{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, RandomCrop, RandomHorizontalFlip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.image_data import ImageClassificationDataset\n",
    "from src.quantize import cluster_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"/mnt/ssd/ronak/datasets/fashion_mnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = FashionMNIST(root, download=True)\n",
    "test_data = FashionMNIST(root, download=True, train=False)\n",
    "\n",
    "x = train_data.data\n",
    "y = np.array(train_data.targets)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 3)\n",
      "(60000,)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "x_train = np.tile(train_data.data[..., None], 3)\n",
    "y_train = np.array(train_data.targets)\n",
    "x_test =  np.tile(test_data.data[..., None], 3)\n",
    "y_test = np.array(test_data.targets)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(len(np.unique(y_train)))\n",
    "\n",
    "np.save(os.path.join(root, \"x_train.npy\"), x_train)\n",
    "np.save(os.path.join(root, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(root, \"x_test.npy\"), x_test)\n",
    "np.save(os.path.join(root, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed with ConvNeXt Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt/ssd/ronak/datasets/fashion_mnist'\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_base(weights=ConvNeXt_Base_Weights.IMAGENET1K_V1).to(DEVICE)\n",
    "# train_nodes, eval_nodes = get_graph_node_names(model) # use to check layer names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "return_nodes = {\n",
    "    # node_name: user-specified key for output dict\n",
    "    'avgpool': 'features',\n",
    "}\n",
    "body = create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "root = DATA_PATH\n",
    "x_train = np.load(os.path.join(root, \"x_train.npy\"))\n",
    "x_train = np.transpose(np.tile(x_train[..., None], 3), axes=[0, 3, 1, 2])\n",
    "y_train = np.load(os.path.join(root, \"y_train.npy\"))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "batch_size = 256\n",
    "transforms = ConvNeXt_Base_Weights.IMAGENET1K_V1.transforms()\n",
    "train_dataset = ImageClassificationDataset(x_train, y_train, transforms)\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "235it [08:24,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "all_image_features, all_labels, all_idx = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(dataloader)):\n",
    "        idx, images, labels = batch\n",
    "        image_features = body(images.to(DEVICE))['features'].squeeze()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        all_image_features.append(image_features)\n",
    "        all_labels.append(labels)\n",
    "        all_idx.append(idx)\n",
    "        \n",
    "all_image_features = torch.cat(all_image_features).cpu().detach().numpy()\n",
    "all_labels = torch.cat(all_labels).cpu().detach().numpy()\n",
    "all_idx = torch.cat(all_idx).cpu().detach().numpy()\n",
    "\n",
    "torch.save(all_image_features, os.path.join(DATA_PATH, \"convnext_base_features.pt\"))\n",
    "torch.save(all_labels, os.path.join(DATA_PATH, \"convnext_base_labels.pt\"))\n",
    "torch.save(all_idx, os.path.join(DATA_PATH, \"convnext_base_idx.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 50\n",
    "SEED = 11182023\n",
    "DATASET = \"fashion_mnist\"\n",
    "DATA_PATH = f'/mnt/ssd/ronak/datasets/{DATASET}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_features = torch.load(os.path.join(DATA_PATH, \"convnext_base_features.pt\"))\n",
    "all_labels = torch.load(os.path.join(DATA_PATH, \"convnext_base_labels.pt\"))\n",
    "all_idx = torch.load(os.path.join(DATA_PATH, \"convnext_base_idx.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 59997 59998 59999]\n",
      "(60000,)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "image_labels, image_cluster = cluster_feat(all_image_features, NUM_CLUSTERS, seed=SEED)\n",
    "\n",
    "label_to_idx = np.argsort(all_idx)\n",
    "print(all_idx[label_to_idx])\n",
    "\n",
    "# have the labels correspond to the indices in order.\n",
    "image_labels_sorted = image_labels[label_to_idx]\n",
    "class_labels_sorted = all_labels[label_to_idx]\n",
    "\n",
    "print(image_labels_sorted.shape)\n",
    "print(class_labels_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"convnext_base\"\n",
    "save_dir = f'/mnt/ssd/ronak/datasets/{DATASET}/quantization/{model_name}_kmeans_{NUM_CLUSTERS}'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_labels.npy'), image_labels_sorted)\n",
    "np.save(os.path.join(save_dir, f'class_labels.npy'), class_labels_sorted)\n",
    "\n",
    "_, counts = np.unique(all_labels, return_counts=True)\n",
    "y_marginal = counts / np.sum(counts)\n",
    "x_marginal = image_cluster.marginal\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_marginal.npy'), x_marginal)\n",
    "np.save(os.path.join(save_dir, f'class_marginal.npy'), y_marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
