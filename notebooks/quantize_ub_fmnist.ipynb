{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.image_data import ImageClassificationDataset\n",
    "from src.quantize import cluster_feat\n",
    "from src.ubmnist import UnbalanceFashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"ub_fmnist\"\n",
    "DATA_PATH = f'/mnt/ssd/ronak/datasets/{DATASET}'\n",
    "root = DATA_PATH\n",
    "MODEL_NAME = \"convmnist_e24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14394, 28, 28)\n",
      "(14394,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronak/resnets/notebooks/../src/ubmnist.py:295: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/mnt/hdd2/liu16/data'\n",
    "val_size = 1000\n",
    "smooth = 0.005\n",
    "size = 14400\n",
    "\n",
    "trainset = UnbalanceFashionMNIST(\n",
    "    root=data_dir, train=True, val_size=val_size,\n",
    "    download=True, transform=transforms.ToTensor(),\n",
    "    smooth=smooth, size=size)\n",
    "mean = (trainset.data.float().mean().item() / 255,)\n",
    "std = (trainset.data.float().std().item() / 255,)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),])\n",
    "\n",
    "# load again using transforms\n",
    "train_data = UnbalanceFashionMNIST(root=data_dir, train=True, val_size=val_size,\n",
    "            download=True, transform=transform, smooth=smooth, size=size)\n",
    "\n",
    "train_data.data = torch.clone(train_data.data).numpy()\n",
    "train_data.targets = torch.clone(train_data.targets).numpy()\n",
    "\n",
    "print(train_data.data.shape)\n",
    "print(train_data.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = FashionMNIST(root, download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14394, 28, 28)\n",
      "(14394,)\n"
     ]
    }
   ],
   "source": [
    "x = train_data.data\n",
    "y = np.array(train_data.targets)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00687787 0.34549118 0.12143949 0.02007781 0.14658886 0.02243991\n",
      " 0.00840628 0.03661248 0.06863971 0.22342643]\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(y, return_counts=True)\n",
    "print(counts / counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0  15  70  92 101 114  93  79  74\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0 122 236 231 237 237 240 235 232 227\n",
      "  226  67   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 211 234 206 190 220 229 231 207 190\n",
      "  211 206   0   0   1   1   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0  28 206 203 209 208 180 215 225 223 153 207\n",
      "  222 209 155   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   4   0  15 201 198 181 174 191 196 196 245 200 202 220\n",
      "  191 177 199 173   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0   0 174 203 178 180 181 180 172 173 225 190 181 172\n",
      "  171 182 172 203 141   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 170 194 184 180 175 179 173 172 235 187 171 179\n",
      "  175 180 177 189 165   0   0   0   0   0]\n",
      " [  0   0   0   0   0   9 195 189 184 185 180 177 176 168 246 182 168 174\n",
      "  173 173 183 182 192  17   0   0   0   0]\n",
      " [  0   0   0   0   0  34 204 185 197 190 173 169 172 168 236 182 165 171\n",
      "  171 179 192 181 198  44   0   0   0   0]\n",
      " [  0   0   0   0   0  86 205 181 195 187 171 176 180 169 237 190 169 174\n",
      "  167 181 190 170 201 126   0   0   0   0]\n",
      " [  0   0   0   0   0 138 202 188 224 209 184 183 180 171 242 193 179 178\n",
      "  175 187 217 188 195 136   0   0   0   0]\n",
      " [  0   0   0   0   0 183 199 197 242 214 186 193 190 179 238 194 177 186\n",
      "  180 188 231 199 192 169   0   0   0   0]\n",
      " [  0   0   0   0   0 215 193 208 232 201 184 184 182 176 236 198 174 179\n",
      "  191 183 237 211 185 211   0   0   0   0]\n",
      " [  0   0   0   0   0 229 192 219 206 194 194 192 192 184 235 203 184 187\n",
      "  191 183 187 221 188 228   1   0   0   0]\n",
      " [  0   0   0   0  13 235 193 229 130 199 194 184 183 174 240 198 172 179\n",
      "  190 193 129 228 191 230  40   0   0   0]\n",
      " [  0   0   0   0  51 205 196 255  66 202 195 188 191 181 250 208 182 190\n",
      "  189 196  68 255 194 201  73   0   0   0]\n",
      " [  0   0   0   0  72 208 202 255  34 215 189 189 189 179 246 204 176 182\n",
      "  185 209  53 253 199 205  93   0   0   0]\n",
      " [  0   0   0   0  92 209 208 239  16 233 181 187 186 178 241 206 180 189\n",
      "  182 226  25 233 204 203 112   0   0   0]\n",
      " [  0   0   0   0 106 205 217 210  20 237 174 190 188 181 242 207 179 189\n",
      "  174 231  37 210 212 201 136   0   0   0]\n",
      " [  0   0   0   0 109 206 223 193  93 232 175 186 185 179 240 207 174 182\n",
      "  169 223  92 183 220 206 138   0   0   0]\n",
      " [  0   0   0   0 123 206 227 146 135 224 183 191 189 188 244 207 182 190\n",
      "  179 212 145 149 239 204 138   0   0   0]\n",
      " [  0   0   0   0 133 207 230 122 167 219 178 189 190 181 246 207 184 184\n",
      "  174 206 173 131 242 205 155   0   0   0]\n",
      " [  0   0   0   0 130 207 233 114 175 198 191 189 188 194 227 207 191 194\n",
      "  186 196 188 127 239 203 152   0   0   0]\n",
      " [  0   0   0   0 130 207 229 141 212 198 193 191 192 189 230 209 187 189\n",
      "  184 184 196 153 223 199 154   0   0   0]\n",
      " [  0   0   0   0 137 207 228 107 189 206 180 182 182 201 215 203 192 193\n",
      "  189 226 231 119 222 201 158   0   0   0]\n",
      " [  0   0   0   0 146 195 230  43   0   0   0   0   0  15   2   1   0   0\n",
      "    3  11   0  50 228 195 167   0   0   0]\n",
      " [  0   0   0   0 198 215 245  80   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  66 232 189 180   0   0   0]\n",
      " [  0   0   0   0  47 150 148   0   0   1   0   1   0   3   2   2   2   2\n",
      "    1   2   0   0 195 223  98   0   0   0]]\n",
      "(14394, 28, 28)\n",
      "(14394,)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# tiling is unnecessary as we will use a grayscale quantization model\n",
    "\n",
    "# x_train = np.tile(train_data.data[..., None], 3) / 255\n",
    "x_train = train_data.data\n",
    "y_train = train_data.targets\n",
    "# x_test =  np.tile(test_data.data[..., None], 3) / 255\n",
    "x_test =  test_data.data\n",
    "y_test = test_data.targets\n",
    "\n",
    "print(x_train[0, :, :])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(len(np.unique(y_train)))\n",
    "\n",
    "np.save(os.path.join(root, \"x_train.npy\"), x_train)\n",
    "np.save(os.path.join(root, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(root, \"x_test.npy\"), x_test)\n",
    "np.save(os.path.join(root, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvMnist(nn.Module):\n",
    "    def __init__(self, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*3*3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 10)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)  # conv1\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # conv2\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2) # conv3 \n",
    "        x = x.view(x.shape[0], -1) # flatten\n",
    "        features = self.fc1(x)\n",
    "        x = F.relu(features)\n",
    "        x = self.fc2(x)\n",
    "        if return_feats:\n",
    "            return x, features\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvMnist(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=576, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/mnt/hdd2/liu16/convnet/unbalance_fashion_mnist_smooth0.005_size14400_v1000_b64/checkpoints/epoch_24.pt'\n",
    "num_hidden = 512\n",
    "\n",
    "record = torch.load(model_path)\n",
    "model = model = ConvMnist(num_hidden)\n",
    "model.load_state_dict(record['state_dict'])\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14394, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "root = DATA_PATH\n",
    "\n",
    "x_train = np.expand_dims(np.load(os.path.join(root, \"x_train.npy\")), axis=1)\n",
    "y_train = np.load(os.path.join(root, \"y_train.npy\"))\n",
    "print(x_train.shape)\n",
    "\n",
    "batch_size = 256\n",
    "transform = transforms.Compose([\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),])\n",
    "train_dataset = ImageClassificationDataset(x_train, y_train, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:01, 50.41it/s]\n"
     ]
    }
   ],
   "source": [
    "all_image_features, all_labels, all_idx = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(dataloader)):\n",
    "        idx, images, labels = batch\n",
    "        image_features = model(images.to(DEVICE), return_feats=True)[1].squeeze()\n",
    "        all_image_features.append(image_features)\n",
    "        all_labels.append(labels)\n",
    "        all_idx.append(idx)\n",
    "        \n",
    "all_image_features = torch.cat(all_image_features).cpu().detach().numpy()\n",
    "all_labels = torch.cat(all_labels).cpu().detach().numpy()\n",
    "all_idx = torch.cat(all_idx).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_image_features, os.path.join(DATA_PATH, f\"{MODEL_NAME}_features.pt\"))\n",
    "torch.save(all_labels, os.path.join(DATA_PATH, f\"{MODEL_NAME}_labels.pt\"))\n",
    "torch.save(all_idx, os.path.join(DATA_PATH, f\"{MODEL_NAME}_idx.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 100\n",
    "SEED = 11182023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_features = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_features.pt\"))\n",
    "all_labels = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_labels.pt\"))\n",
    "all_idx = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_idx.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 14391 14392 14393]\n",
      "(14394,)\n",
      "(14394,)\n"
     ]
    }
   ],
   "source": [
    "image_labels, image_cluster = cluster_feat(all_image_features, NUM_CLUSTERS, seed=SEED)\n",
    "\n",
    "label_to_idx = np.argsort(all_idx)\n",
    "print(all_idx[label_to_idx])\n",
    "\n",
    "# have the labels correspond to the indices in order.\n",
    "image_labels_sorted = image_labels[label_to_idx]\n",
    "class_labels_sorted = all_labels[label_to_idx]\n",
    "\n",
    "print(image_labels_sorted.shape)\n",
    "print(class_labels_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'/mnt/ssd/ronak/datasets/{DATASET}/quantization/{MODEL_NAME}_kmeans_{NUM_CLUSTERS}'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_labels.npy'), image_labels_sorted)\n",
    "np.save(os.path.join(save_dir, f'class_labels.npy'), class_labels_sorted)\n",
    "\n",
    "_, counts = np.unique(all_labels, return_counts=True)\n",
    "y_marginal = counts/np.sum(counts)\n",
    "x_marginal = image_cluster.marginal\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_marginal.npy'), x_marginal)\n",
    "np.save(os.path.join(save_dir, f'class_marginal.npy'), y_marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
