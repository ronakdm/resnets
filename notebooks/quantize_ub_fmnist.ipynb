{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.image_data import ImageClassificationDataset\n",
    "from src.quantize import cluster_feat, KMeans\n",
    "from src.ubmnist import UnbalanceFashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"ub_fmnist\"\n",
    "DATA_PATH = f'/mnt/ssd/ronak/datasets/{DATASET}'\n",
    "root = DATA_PATH\n",
    "MODEL_NAME = \"convmnist_e24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/mnt/hdd2/liu16/data'\n",
    "val_size = 1000\n",
    "smooth = 0.005\n",
    "size = 14400\n",
    "\n",
    "trainset = UnbalanceFashionMNIST(\n",
    "    root=data_dir, train=True, val_size=val_size,\n",
    "    download=True, transform=transforms.ToTensor(),\n",
    "    smooth=smooth, size=size)\n",
    "mean = (trainset.data.float().mean().item() / 255,)\n",
    "std = (trainset.data.float().std().item() / 255,)\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),])\n",
    "\n",
    "# load again using transforms\n",
    "train_data = UnbalanceFashionMNIST(root=data_dir, train=True, val_size=val_size,\n",
    "            download=True, transform=transform, smooth=smooth, size=size)\n",
    "# train_data = UnbalanceFashionMNIST(\n",
    "#     root=data_dir, train=True, val_size=val_size,\n",
    "#     download=True, transform=transforms.ToTensor(),\n",
    "#     smooth=smooth, size=size)\n",
    "test_data = FashionMNIST(root, download=True, train=False)\n",
    "\n",
    "x_train = torch.clone(train_data.data).numpy()\n",
    "y_train = torch.clone(train_data.targets).numpy()\n",
    "x_test = torch.clone(test_data.data).numpy()\n",
    "y_test = torch.clone(test_data.targets).numpy()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mean = mean[0] * 255\n",
    "new_std = std[0] * 255\n",
    "\n",
    "x_train.mean() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data.data\n",
    "y = np.array(train_data.targets)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(y, return_counts=True)\n",
    "print(counts / counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiling is unnecessary as we will use a grayscale quantization model\n",
    "\n",
    "# x_train = np.tile(train_data.data[..., None], 3) / 255\n",
    "x_train = (x_train - new_mean) / new_std * 255\n",
    "y_train = train_data.targets\n",
    "# x_test =  np.tile(test_data.data[..., None], 3) / 255\n",
    "x_test =  (test_data.data - new_mean) / new_std * 255\n",
    "y_test = test_data.targets\n",
    "\n",
    "print(x_train[0, :, :])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(len(np.unique(y_train)))\n",
    "\n",
    "np.save(os.path.join(root, \"x_train.npy\"), x_train)\n",
    "np.save(os.path.join(root, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(root, \"x_test.npy\"), x_test)\n",
    "np.save(os.path.join(root, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = x_train[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvMnist(nn.Module):\n",
    "    def __init__(self, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*3*3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 10)\n",
    "\n",
    "    def forward(self, x, return_feats=False):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)  # conv1\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # conv2\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2) # conv3 \n",
    "        x = x.view(x.shape[0], -1) # flatten\n",
    "        features = self.fc1(x)\n",
    "        x = F.relu(features)\n",
    "        x = self.fc2(x)\n",
    "        if return_feats:\n",
    "            return x, features\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvMnist(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=576, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/mnt/hdd2/liu16/convnet/unbalance_fashion_mnist_smooth0.005_size14400_v1000_b256/checkpoints/epoch_24.pt'\n",
    "num_hidden = 512\n",
    "\n",
    "record = torch.load(model_path)\n",
    "model = model = ConvMnist(num_hidden)\n",
    "model.load_state_dict(record['state_dict'])\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401\n",
      "  -0.80041401 -0.80041401 -0.80041401 -0.80041401]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401\n",
      "  -0.80041401 -0.78959127 -0.80041401  0.51996042]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401\n",
      "  -0.80041401 -0.80041401 -0.80041401  1.48318438]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.77876853\n",
      "  -0.80041401 -0.49737726  1.42907068  1.39660245]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.75712305 -0.80041401\n",
      "  -0.63807289  1.37495697  1.34248875  1.15850215]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401\n",
      "   1.08274296  1.39660245  1.12603392  1.14767941]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401\n",
      "   1.03945199  1.29919778  1.19097037  1.14767941]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.70300934\n",
      "   1.31002052  1.24508408  1.19097037  1.20179311]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.43244081\n",
      "   1.40742519  1.20179311  1.33166601  1.25590682]\n",
      " [-0.80041401 -0.80041401 -0.80041401 -0.80041401 -0.80041401  0.13034173\n",
      "   1.41824794  1.15850215  1.31002052  1.22343859]]\n",
      "(14394, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "root = DATA_PATH\n",
    "\n",
    "x_train = np.expand_dims(np.load(os.path.join(root, \"x_train.npy\")), axis=1) / 255\n",
    "print(x_train[0, 0, :10, :10])\n",
    "y_train = np.load(os.path.join(root, \"y_train.npy\"))\n",
    "print(x_train.shape)\n",
    "\n",
    "batch_size = 256\n",
    "# transform = transforms.Compose([\n",
    "#             # transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean, std),])\n",
    "# train_dataset = ImageClassificationDataset(x_train, y_train, transform=transform)\n",
    "train_dataset = ImageClassificationDataset(x_train, y_train)\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:00, 136.11it/s]\n"
     ]
    }
   ],
   "source": [
    "all_image_features, all_labels, all_idx = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(dataloader)):\n",
    "        idx, images, labels = batch\n",
    "        image_features = model(images.to(DEVICE), return_feats=True)[1].squeeze()\n",
    "        all_image_features.append(image_features)\n",
    "        all_labels.append(labels)\n",
    "        all_idx.append(idx)\n",
    "        \n",
    "all_image_features = torch.cat(all_image_features).cpu().detach().numpy()\n",
    "all_labels = torch.cat(all_labels).cpu().detach().numpy()\n",
    "all_idx = torch.cat(all_idx).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_image_features, os.path.join(DATA_PATH, f\"{MODEL_NAME}_features.pt\"))\n",
    "torch.save(all_labels, os.path.join(DATA_PATH, f\"{MODEL_NAME}_labels.pt\"))\n",
    "torch.save(all_idx, os.path.join(DATA_PATH, f\"{MODEL_NAME}_idx.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 100\n",
    "SEED = 20220711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_features = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_features.pt\"))\n",
    "all_labels = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_labels.pt\"))\n",
    "all_idx = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_idx.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 14391 14392 14393]\n",
      "(14394,)\n",
      "(14394,)\n"
     ]
    }
   ],
   "source": [
    "image_labels, image_cluster = cluster_feat(all_image_features, NUM_CLUSTERS, seed=SEED)\n",
    "\n",
    "label_to_idx = np.argsort(all_idx)\n",
    "print(all_idx[label_to_idx])\n",
    "\n",
    "# have the labels correspond to the indices in order.\n",
    "image_labels_sorted = image_labels[label_to_idx]\n",
    "class_labels_sorted = all_labels[label_to_idx]\n",
    "\n",
    "print(image_labels_sorted.shape)\n",
    "print(class_labels_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"convmnist_e24_train\"\n",
    "save_dir = f'/mnt/ssd/ronak/datasets/{DATASET}/quantization/{model_name}_kmeans_{NUM_CLUSTERS}'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_labels.npy'), image_labels_sorted)\n",
    "np.save(os.path.join(save_dir, f'class_labels.npy'), class_labels_sorted)\n",
    "\n",
    "_, counts = np.unique(all_labels, return_counts=True)\n",
    "y_marginal = counts/np.sum(counts)\n",
    "x_marginal = image_cluster.marginal\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_marginal.npy'), x_marginal)\n",
    "np.save(os.path.join(save_dir, f'class_marginal.npy'), y_marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_marginal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_features = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_features.pt\"))\n",
    "all_labels = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_labels.pt\"))\n",
    "all_idx = torch.load(os.path.join(DATA_PATH, f\"{MODEL_NAME}_idx.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 0.24.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "smooth = 0.005\n",
    "size = 14400\n",
    "epoch = 24\n",
    "val_size = 1000\n",
    "batch_size = 256\n",
    "NUM_CLUSTERS = 100\n",
    "\n",
    "model_name = f'unbalance_fashion_mnist_smooth{smooth}_size{size}_v{val_size}_b{batch_size}'\n",
    "CKP_PATH = f'/mnt/hdd2/liu16/convnet'\n",
    "\n",
    "# cluster_path = f'{CKP_PATH}/{model_name}/kmeans_c{NUM_CLUSTERS}_v{val_size}_e{epoch}_image.pkl'  # remove _v{val_size}\n",
    "cluster_path = f'kmeans_c{NUM_CLUSTERS}_v{val_size}_e{epoch}_image.pkl'  # remove _v{val_size}\n",
    "cluster_path\n",
    "with open(cluster_path, 'rb+') as f:\n",
    "    image_cluster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.quantize.KMeans"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels = image_cluster.clustering(all_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(image_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_cluster.marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 14391 14392 14393]\n",
      "(14394,)\n",
      "(14394,)\n"
     ]
    }
   ],
   "source": [
    "label_to_idx = np.argsort(all_idx)\n",
    "print(all_idx[label_to_idx])\n",
    "\n",
    "# have the labels correspond to the indices in order.\n",
    "image_labels_sorted = image_labels[label_to_idx]\n",
    "class_labels_sorted = all_labels[label_to_idx]\n",
    "\n",
    "print(image_labels_sorted.shape)\n",
    "print(class_labels_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'/mnt/ssd/ronak/datasets/{DATASET}/quantization/{MODEL_NAME}_kmeans_{NUM_CLUSTERS}'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_labels.npy'), image_labels_sorted)\n",
    "np.save(os.path.join(save_dir, f'class_labels.npy'), class_labels_sorted)\n",
    "\n",
    "_, counts = np.unique(all_labels, return_counts=True)\n",
    "y_marginal = counts/np.sum(counts)\n",
    "x_marginal = image_cluster.marginal\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_marginal.npy'), x_marginal)\n",
    "np.save(os.path.join(save_dir, f'class_marginal.npy'), y_marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/ssd/ronak/datasets/ub_fmnist/quantization/convmnist_e24_kmeans_100'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNext Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "from torch.utils.data import DataLoader, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"ub_fmnist\"\n",
    "DATA_PATH = f'/mnt/ssd/ronak/datasets/{DATASET}'\n",
    "root = DATA_PATH\n",
    "DEVICE = \"cuda:2\"\n",
    "MODEL_NAME = \"convnext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "model = convnext_base(weights=ConvNeXt_Base_Weights.IMAGENET1K_V1).to(DEVICE)\n",
    "# train_nodes, eval_nodes = get_graph_node_names(model) # use to check layer names\n",
    "return_nodes = {\n",
    "    # node_name: user-specified key for output dict\n",
    "    'avgpool': 'features',\n",
    "}\n",
    "body = create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.95668029785156\n",
      "92.39803314208984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronak/resnets/notebooks/../src/ubmnist.py:295: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/mnt/hdd2/liu16/data'\n",
    "val_size = 1000\n",
    "smooth = 0.005\n",
    "size = 14400\n",
    "\n",
    "trainset = UnbalanceFashionMNIST(\n",
    "    root=data_dir, train=True, val_size=val_size,\n",
    "    download=True, transform=transforms.ToTensor(),\n",
    "    smooth=smooth, size=size)\n",
    "mean = trainset.data.float().mean().item()\n",
    "std = trainset.data.float().std().item()\n",
    "\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14394, 3, 28, 28)\n",
      "(14394,)\n",
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  15.  70.  92. 101.\n",
      "  114.  93.  79.  74.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   1.   0. 122. 236. 231. 237. 237.\n",
      "  240. 235. 232. 227. 226.  67.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0. 211. 234. 206. 190. 220.\n",
      "  229. 231. 207. 190. 211. 206.   0.   0.   1.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   2.   0.  28. 206. 203. 209. 208. 180. 215.\n",
      "  225. 223. 153. 207. 222. 209. 155.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   4.   0.  15. 201. 198. 181. 174. 191. 196. 196.\n",
      "  245. 200. 202. 220. 191. 177. 199. 173.   0.   0.   2.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 174. 203. 178. 180. 181. 180. 172. 173.\n",
      "  225. 190. 181. 172. 171. 182. 172. 203. 141.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 170. 194. 184. 180. 175. 179. 173. 172.\n",
      "  235. 187. 171. 179. 175. 180. 177. 189. 165.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   9. 195. 189. 184. 185. 180. 177. 176. 168.\n",
      "  246. 182. 168. 174. 173. 173. 183. 182. 192.  17.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.  34. 204. 185. 197. 190. 173. 169. 172. 168.\n",
      "  236. 182. 165. 171. 171. 179. 192. 181. 198.  44.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.  86. 205. 181. 195. 187. 171. 176. 180. 169.\n",
      "  237. 190. 169. 174. 167. 181. 190. 170. 201. 126.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0. 138. 202. 188. 224. 209. 184. 183. 180. 171.\n",
      "  242. 193. 179. 178. 175. 187. 217. 188. 195. 136.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0. 183. 199. 197. 242. 214. 186. 193. 190. 179.\n",
      "  238. 194. 177. 186. 180. 188. 231. 199. 192. 169.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0. 215. 193. 208. 232. 201. 184. 184. 182. 176.\n",
      "  236. 198. 174. 179. 191. 183. 237. 211. 185. 211.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0. 229. 192. 219. 206. 194. 194. 192. 192. 184.\n",
      "  235. 203. 184. 187. 191. 183. 187. 221. 188. 228.   1.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  13. 235. 193. 229. 130. 199. 194. 184. 183. 174.\n",
      "  240. 198. 172. 179. 190. 193. 129. 228. 191. 230.  40.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  51. 205. 196. 255.  66. 202. 195. 188. 191. 181.\n",
      "  250. 208. 182. 190. 189. 196.  68. 255. 194. 201.  73.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  72. 208. 202. 255.  34. 215. 189. 189. 189. 179.\n",
      "  246. 204. 176. 182. 185. 209.  53. 253. 199. 205.  93.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  92. 209. 208. 239.  16. 233. 181. 187. 186. 178.\n",
      "  241. 206. 180. 189. 182. 226.  25. 233. 204. 203. 112.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 106. 205. 217. 210.  20. 237. 174. 190. 188. 181.\n",
      "  242. 207. 179. 189. 174. 231.  37. 210. 212. 201. 136.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 109. 206. 223. 193.  93. 232. 175. 186. 185. 179.\n",
      "  240. 207. 174. 182. 169. 223.  92. 183. 220. 206. 138.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 123. 206. 227. 146. 135. 224. 183. 191. 189. 188.\n",
      "  244. 207. 182. 190. 179. 212. 145. 149. 239. 204. 138.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 133. 207. 230. 122. 167. 219. 178. 189. 190. 181.\n",
      "  246. 207. 184. 184. 174. 206. 173. 131. 242. 205. 155.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 130. 207. 233. 114. 175. 198. 191. 189. 188. 194.\n",
      "  227. 207. 191. 194. 186. 196. 188. 127. 239. 203. 152.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 130. 207. 229. 141. 212. 198. 193. 191. 192. 189.\n",
      "  230. 209. 187. 189. 184. 184. 196. 153. 223. 199. 154.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 137. 207. 228. 107. 189. 206. 180. 182. 182. 201.\n",
      "  215. 203. 192. 193. 189. 226. 231. 119. 222. 201. 158.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 146. 195. 230.  43.   0.   0.   0.   0.   0.  15.\n",
      "    2.   1.   0.   0.   3.  11.   0.  50. 228. 195. 167.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 198. 215. 245.  80.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.  66. 232. 189. 180.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  47. 150. 148.   0.   0.   1.   0.   1.   0.   3.\n",
      "    2.   2.   2.   2.   1.   2.   0.   0. 195. 223.  98.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "root = DATA_PATH\n",
    "# x_train = np.transpose(np.load(os.path.join(root, \"x_train.npy\")), axes=[0, 3, 1, 2])\n",
    "x_train = std * np.load(os.path.join(root, \"x_train.npy\")) / 255 + mean\n",
    "x_train = np.transpose(np.tile(x_train[..., None], 3), axes=[0, 3, 1, 2])\n",
    "y_train = np.load(os.path.join(root, \"y_train.npy\"))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train[0, 0])\n",
    "\n",
    "batch_size = 256\n",
    "transforms_ = ConvNeXt_Base_Weights.IMAGENET1K_V1.transforms()\n",
    "train_dataset = ImageClassificationDataset(x_train, y_train, transforms_)\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "57it [01:56,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "all_image_features, all_labels, all_idx = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(dataloader)):\n",
    "        idx, images, labels = batch\n",
    "        image_features = body(images.to(DEVICE))['features'].squeeze()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        all_image_features.append(image_features)\n",
    "        all_labels.append(labels)\n",
    "        all_idx.append(idx)\n",
    "        \n",
    "all_image_features = torch.cat(all_image_features).cpu().detach().numpy()\n",
    "all_labels = torch.cat(all_labels).cpu().detach().numpy()\n",
    "all_idx = torch.cat(all_idx).cpu().detach().numpy()\n",
    "\n",
    "torch.save(all_image_features, os.path.join(DATA_PATH, \"convnext_base_features.pt\"))\n",
    "torch.save(all_labels, os.path.join(DATA_PATH, \"convnext_base_labels.pt\"))\n",
    "torch.save(all_idx, os.path.join(DATA_PATH, \"convnext_base_idx.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 14391 14392 14393]\n",
      "(14394,)\n",
      "(14394,)\n",
      "/mnt/ssd/ronak/datasets/ub_fmnist/quantization/convnext_base_kmeans_100\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 100\n",
    "SEED = 20220711\n",
    "\n",
    "image_labels, image_cluster = cluster_feat(all_image_features, NUM_CLUSTERS, seed=SEED)\n",
    "\n",
    "label_to_idx = np.argsort(all_idx)\n",
    "print(all_idx[label_to_idx])\n",
    "\n",
    "# have the labels correspond to the indices in order.\n",
    "image_labels_sorted = image_labels[label_to_idx]\n",
    "class_labels_sorted = all_labels[label_to_idx]\n",
    "\n",
    "print(image_labels_sorted.shape)\n",
    "print(class_labels_sorted.shape)\n",
    "\n",
    "model_name = \"convnext_base\"\n",
    "save_dir = f'/mnt/ssd/ronak/datasets/{DATASET}/quantization/{model_name}_kmeans_{NUM_CLUSTERS}'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_labels.npy'), image_labels_sorted)\n",
    "np.save(os.path.join(save_dir, f'class_labels.npy'), class_labels_sorted)\n",
    "\n",
    "_, counts = np.unique(all_labels, return_counts=True)\n",
    "y_marginal = counts/np.sum(counts)\n",
    "x_marginal = image_cluster.marginal\n",
    "\n",
    "np.save(os.path.join(save_dir, f'image_marginal.npy'), x_marginal)\n",
    "np.save(os.path.join(save_dir, f'class_marginal.npy'), y_marginal)\n",
    "\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
