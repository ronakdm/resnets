{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd9f3649e2247ac86c89eaf64a99c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405f6819e9a847d39f587f058c198f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.6647047996521\n",
      "I am having very desperate sevces issues and I am hoping to try to transfer from the SHO into French Point but I'm sure there's to be\n"
     ]
    }
   ],
   "source": [
    "# Start with start text.\n",
    "starter_text = \"I am\"\n",
    "\n",
    "generated = tokenizer.encode(starter_text)\n",
    "\n",
    "# Start with beginning of sentence token.\n",
    "# generated = [0]\n",
    "\n",
    "context = torch.tensor([generated]).to(device)\n",
    "past = None\n",
    "\n",
    "N = 30\n",
    "L = torch.full((1,), 0, dtype=torch.float32).to(device)\n",
    "for i in range(N):\n",
    "\n",
    "  # context shape: (batch_size, seq_len)\n",
    "  with torch.no_grad():\n",
    "    output = model(context, past_key_values=past)\n",
    "    logits = output.logits # (batch_size, seq_len, vocab_len)\n",
    "    past = output.past_key_values \n",
    "\n",
    "    probs = F.softmax(logits[:,-1,:], dim=-1)\n",
    "    token = torch.multinomial(probs, num_samples=1)\n",
    "    token_int = token[0].item()\n",
    "\n",
    "    generated += [token_int]\n",
    "    logprob = torch.log(probs[0,token_int])\n",
    "    L += logprob\n",
    "    context = token\n",
    "\n",
    "sequence = tokenizer.decode(generated)\n",
    "\n",
    "print(float(L/N))\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I WOULD NOT LIKE THEM HERE OR THERE. I WOULD NOT LIKE THEM ANYWHERE. I DO NOT LIKE GREEN EGGS AND HAM. I DO NOT LIKE THEM, SAM-I-AM.\"\n",
    "x = torch.tensor([tokenizer.encode(text)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters())\n",
    "grads = []\n",
    "\n",
    "out = model(x, labels=x)\n",
    "ll = -out.loss\n",
    "ll.backward()\n",
    "\n",
    "for i, p in enumerate(params):\n",
    "    grads.append(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([1024, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for grad in grads:\n",
    "    print(grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optostim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
