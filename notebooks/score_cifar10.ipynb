{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.image_data import ImageClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt/ssd/ronak/datasets/cifar10'\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_shapes(parameters):\n",
    "    return [torch.tensor(p.shape) for p in parameters]\n",
    "\n",
    "def get_num_parameters(param_shapes):\n",
    "    return torch.tensor([torch.prod(s) for s in param_shapes]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_base(weights=ConvNeXt_Base_Weights.IMAGENET1K_V1).to(DEVICE)\n",
    "# train_nodes, eval_nodes = get_graph_node_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in foundation model: 88591464\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of parameters in foundation model: {get_num_parameters(get_param_shapes(model.parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "[[ 59  16  25  33  50  71  97 115 137 154 154 145 142 158 145 148 149 147\n",
      "  152 145 143 143 141 143 149 172 202 216 220 208 180 177]\n",
      " [ 62  20  24  25  32  48  69  82 100 120 122 114 115 131 115 116 115 111\n",
      "  114 105 104 104 102 103 107 128 157 174 182 170 139 144]\n",
      " [ 63  20  21  17  21  29  40  49  68  89  94  89  86  98  79  79  79  76\n",
      "   80  72  66  64  65  72  74  76  82  87  91  96  96 116]]\n"
     ]
    }
   ],
   "source": [
    "root = DATA_PATH\n",
    "\n",
    "x_train = np.transpose(np.load(os.path.join(root, \"x_train.npy\")), axes=[0, 3, 1, 2])\n",
    "y_train = np.load(os.path.join(root, \"y_train.npy\"))\n",
    "print(x_train.shape)\n",
    "print(x_train[0, :, :, 0])\n",
    "\n",
    "batch_size = 1\n",
    "transforms = ConvNeXt_Base_Weights.IMAGENET1K_V1.transforms()\n",
    "train_dataset = ImageClassificationDataset(x_train, y_train, transforms)\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ronak/miniconda3/envs/dl/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_image_features, all_idx = [], []\n",
    "with torch.no_grad():\n",
    "    for i, batch in tqdm(enumerate(dataloader)):\n",
    "        idx, images, labels = batch\n",
    "        output = model(images.to(DEVICE))\n",
    "        break\n",
    "        # all_image_features.append(image_features)\n",
    "        all_idx.append(idx)\n",
    "        \n",
    "# all_image_features = torch.cat(all_image_features).cpu().detach().numpy()\n",
    "# all_idx = torch.cat(all_idx).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
