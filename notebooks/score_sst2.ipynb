{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "# from src.text_data import TokenizedTextClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"sst2\"\n",
    "DATA_PATH = f\"/mnt/ssd/ronak/datasets/{DATASET}\"\n",
    "DEVICE = 'cuda:0'\n",
    "MODEL_NAME = f\"gpt2\"\n",
    "SEED = 11182023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train = load_dataset('sst2', split='train', cache_dir=DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_shapes(parameters):\n",
    "    return [torch.tensor(p.shape) for p in parameters]\n",
    "\n",
    "def get_num_parameters(param_shapes):\n",
    "    return torch.tensor([torch.prod(s) for s in param_shapes]).sum().item()\n",
    "\n",
    "def flatten_gradient(grads):\n",
    "    return torch.cat([p.reshape(-1) for p in grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in foundation model: 124439808\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained(f'gpt2-{MODEL_SIZE}')\n",
    "# model = GPT2LMHeadModel.from_pretrained(f'gpt2-{MODEL_SIZE}').to(DEVICE)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(f'gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained(f'gpt2').to(DEVICE)\n",
    "print(f\"number of parameters in foundation model: {get_num_parameters(get_param_shapes(model.parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [00:11<00:00, 5645.68it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for sent in tqdm(train['sentence']):\n",
    "    encoded_input = tokenizer(sent, return_tensors='pt')\n",
    "    texts.append(encoded_input['input_ids'].to(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Model and Compute Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 12\n",
    "n_parameters = get_num_parameters(get_param_shapes(model.parameters()))\n",
    "np.random.seed(123)\n",
    "rand_project = np.random.normal(size=(n_components, n_parameters)).astype(np.float16) / np.sqrt(n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'memory at 32000 n_components: 7964 GB'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 32000\n",
    "f\"memory at {n_components} n_components: {int(n_components * n_parameters * 2 / 1e9):04d} GB\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:56, 11.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mtext, labels\u001b[38;5;241m=\u001b[39mtext, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     g_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mloss, inputs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m----> 9\u001b[0m grads\u001b[38;5;241m.\u001b[39mappend(rand_project \u001b[38;5;241m@\u001b[39m flatten_gradient(g_out)\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2000\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    grads = []\n",
    "    for i, text in tqdm(enumerate(texts)):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        with torch.enable_grad():\n",
    "            output = model(input_ids=text, labels=text, output_hidden_states=False, output_attentions=False)\n",
    "            g_out = torch.autograd.grad(outputs=output.loss, inputs=model.parameters())\n",
    "        grads.append(rand_project @ flatten_gradient(g_out).half().detach().cpu().numpy())\n",
    "        if i > 2000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n"
     ]
    }
   ],
   "source": [
    "grads = np.stack(grads)\n",
    "print(grads.shape)\n",
    "np.save(os.path.join(DATA_PATH, f\"{MODEL_NAME}_scores.npy\"), grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.07  ,  48.78  , -10.84  , -12.93  ,  25.67  , -24.38  ,\n",
       "         -1.136 ,  39.03  ,  26.98  , -29.83  , -11.02  , -22.98  ],\n",
       "       [ 20.52  ,  17.73  ,  -8.59  ,  25.83  ,  -3.47  , -18.31  ,\n",
       "         -1.494 ,  25.42  ,   6.164 ,  -4.277 , -31.89  , -19.78  ],\n",
       "       [  8.97  ,  -7.6   ,   6.887 , -54.1   , -25.97  ,  15.5   ,\n",
       "         11.2   ,   2.47  , -15.69  ,   0.8696,  -7.473 , -29.27  ],\n",
       "       [ -9.55  ,  33.88  ,   1.843 ,  27.84  ,  31.34  ,   1.576 ,\n",
       "         27.06  ,   9.8   ,   2.244 , -13.74  ,  12.19  ,  24.7   ],\n",
       "       [ -5.08  ,   6.668 ,   1.905 ,  21.84  ,   4.645 ,   5.27  ,\n",
       "          8.9   ,   2.875 ,  16.14  ,  -0.1884,  -2.309 ,   2.852 ],\n",
       "       [  7.434 , -10.58  ,   5.49  ,   1.426 ,  12.266 ,  23.55  ,\n",
       "        -18.55  , -21.88  ,  -0.342 ,   0.1289, -11.49  ,  -3.13  ]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
