{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.models import MyrtleNet\n",
    "from src.image_data import get_cifar10_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_cfg = {\n",
    "    \"architecture\": \"myrtle_net\",\n",
    "    \"n_layers\": 3,\n",
    "    \"residual_blocks\": [0, 2],\n",
    "}\n",
    "model = MyrtleNet(**model_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50,000 training samples.\n",
      "10,000 test samples.\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "batch_size = 512\n",
    "root = \"/mnt/ssd/ronak/datasets/\"\n",
    "\n",
    "train_loader, val_loader = get_cifar10_loaders(batch_size, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim\n",
    "max_iters = 500\n",
    "optim_cfg = {\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.4,\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.0,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=optim_cfg[\"lr\"],\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    total_steps=max_iters,\n",
    "    anneal_strategy=\"linear\",\n",
    "    pct_start=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss\n",
      "-------------\n",
      "000: 2.6876\n",
      "020: 5.2998\n",
      "040: 6.2296\n",
      "060: 5.1665\n",
      "080: 7.1584\n",
      "100: 4.5171\n",
      "120: 4.2654\n",
      "140: 5.1073\n",
      "160: 7.9966\n",
      "Graceful Exit\n"
     ]
    }
   ],
   "source": [
    "# Run experiment.\n",
    "grad_accumulation_steps = 1\n",
    "print_interval = 20\n",
    "\n",
    "model.train()\n",
    "iter_num = 0\n",
    "print(\"Training Loss\")\n",
    "print(\"-------------\")\n",
    "try:\n",
    "    while iter_num < max_iters:\n",
    "        for X, Y in train_loader:\n",
    "            loss, logits = model(X.to(device), Y.to(device))\n",
    "            loss = loss / grad_accumulation_steps\n",
    "            if iter_num % print_interval == 0:\n",
    "                print(f\"{iter_num:03d}: {loss.item():0.4f}\")\n",
    "            loss.backward()\n",
    "            if iter_num % grad_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            iter_num += 1\n",
    "            if iter_num > max_iters:\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Graceful Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
