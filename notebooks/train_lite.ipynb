{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.models import MyrtleNet\n",
    "from src.image_data import get_cifar10_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_cfg = {\n",
    "    \"architecture\": \"myrtle_net\",\n",
    "    \"n_layers\": 3,\n",
    "    \"residual_blocks\": [0, 2],\n",
    "}\n",
    "model = MyrtleNet(**model_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50,000 training samples.\n",
      "10,000 test samples.\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "batch_size = 512\n",
    "root = \"/mnt/ssd/ronak/datasets/\"\n",
    "\n",
    "train_loader, val_loader = get_cifar10_loaders(batch_size, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim\n",
    "optim_cfg = {\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.003,\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=optim_cfg[\"lr\"], weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss\n",
      "-------------\n",
      "000: 3.0742\n",
      "001: 9.7745\n",
      "002: 11.2783\n",
      "003: 7.6125\n",
      "004: 9.0296\n",
      "005: 8.1771\n",
      "006: 6.2588\n",
      "007: 6.4001\n",
      "008: 3.6303\n",
      "009: 4.9894\n",
      "010: 4.4825\n",
      "011: 4.5366\n",
      "012: 5.3237\n",
      "013: 4.3536\n",
      "014: 4.0704\n",
      "015: 3.2507\n",
      "016: 4.1695\n",
      "017: 3.7187\n",
      "018: 3.3210\n",
      "019: 3.4755\n",
      "020: 3.2531\n",
      "021: 3.8041\n",
      "022: 3.6164\n",
      "023: 2.9680\n",
      "024: 2.7723\n",
      "025: 2.8352\n",
      "026: 2.6664\n",
      "027: 2.5693\n",
      "028: 2.6800\n",
      "029: 2.9628\n",
      "030: 2.9264\n",
      "031: 2.5191\n",
      "032: 2.7009\n",
      "033: 2.3546\n",
      "034: 2.6264\n",
      "035: 2.3360\n",
      "036: 2.2495\n",
      "037: 2.5527\n",
      "038: 2.5076\n",
      "039: 2.5318\n",
      "040: 2.3609\n",
      "041: 2.2626\n",
      "042: 2.2117\n",
      "043: 2.3234\n",
      "044: 2.2248\n",
      "045: 2.2021\n",
      "046: 2.0719\n",
      "047: 2.0554\n",
      "048: 2.2171\n",
      "049: 2.0895\n",
      "050: 2.1538\n",
      "051: 2.1404\n",
      "052: 2.0683\n",
      "053: 2.0458\n",
      "054: 2.1160\n",
      "055: 1.9735\n",
      "056: 1.9837\n",
      "057: 2.0055\n",
      "058: 1.9490\n",
      "059: 1.9669\n",
      "060: 1.9862\n",
      "061: 1.9782\n",
      "062: 2.0079\n",
      "063: 1.9301\n",
      "064: 2.0060\n",
      "065: 1.9790\n",
      "066: 1.8536\n",
      "067: 1.9174\n",
      "068: 1.9157\n",
      "069: 1.8783\n",
      "070: 1.8009\n",
      "071: 1.8293\n",
      "072: 1.8390\n",
      "073: 1.8228\n",
      "074: 1.8456\n",
      "075: 1.9386\n",
      "076: 1.7733\n",
      "077: 1.8697\n",
      "078: 1.8332\n",
      "079: 1.8824\n",
      "080: 1.8055\n",
      "081: 1.8109\n",
      "082: 1.9450\n",
      "083: 1.8766\n",
      "084: 1.8432\n",
      "085: 1.7975\n",
      "086: 1.7074\n"
     ]
    }
   ],
   "source": [
    "# Run experiment.\n",
    "max_iters = 100\n",
    "grad_accumulation_steps = 1\n",
    "\n",
    "model.train()\n",
    "iter_num = 0\n",
    "print(\"Training Loss\")\n",
    "print(\"-------------\")\n",
    "while iter_num < max_iters:\n",
    "    for X, Y in train_loader:\n",
    "        loss, logits = model(X.to(device), Y.to(device))\n",
    "        loss = loss / grad_accumulation_steps\n",
    "        print(f\"{iter_num:03d}: {loss.item():0.4f}\")\n",
    "        loss.backward()\n",
    "        if iter_num % grad_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        iter_num += 1\n",
    "        if iter_num > max_iters:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
